{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.]], requires_grad=True)\n",
      "tensor(4., grad_fn=<MseLossBackward0>)\n",
      "tensor([[4.]])\n",
      "tensor(4., grad_fn=<MseLossBackward0>)\n",
      "tensor([[8.]])\n",
      "tensor(4., grad_fn=<MseLossBackward0>)\n",
      "tensor([[4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([[2.0]], requires_grad=True)\n",
    "x = torch.tensor([[1.0]])\n",
    "x2 = torch.tensor([[1.0], [1.0]])\n",
    "target = torch.tensor([[0.0]])\n",
    "target2 = torch.tensor([[0.0], [0.0]])\n",
    "\n",
    "def model(x, w):\n",
    "    return x.mm(w)\n",
    "\n",
    "\n",
    "loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "print(w)\n",
    "loss1 = loss(model(x, w), target)\n",
    "print(loss1)\n",
    "\n",
    "loss1.backward()\n",
    "print(w.grad)\n",
    "\n",
    "loss1 = loss(model(x, w), target)\n",
    "print(loss1)\n",
    "\n",
    "loss1.backward()\n",
    "print(w.grad)\n",
    "\n",
    "w = torch.tensor([[2.0]], requires_grad=True)\n",
    "\n",
    "loss2 = loss(model(x2, w), target2)\n",
    "print(loss2)\n",
    "\n",
    "loss2.backward()\n",
    "print(w.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "import qtorch\n",
    "from qtorch.quant import Quantizer\n",
    "import qtorch\n",
    "import tqdm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch_size', type=int, default=64)\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--man_width', type=int, default=5)\n",
    "parser.add_argument('--group', type=str)\n",
    "parser.add_argument('--id', type=str)\n",
    "parser.add_argument(\"--round_mode\", type=str, default=\"stochastic\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "# %%\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "MnistDataset = datasets.MNIST(\"./data\", download=True, train=False, transform=transform)\n",
    "\n",
    "MnistDatasetTrain =torch.utils.data.Subset(MnistDataset, range(7500)) \n",
    "MnistDatasetTest =torch.utils.data.Subset(MnistDataset, range(7500, 10000)) \n",
    "fnumber = qtorch.FloatingPoint(8, args.man_width)\n",
    "bnumber = qtorch.FloatingPoint(8, args.man_width)\n",
    "\n",
    "def get_network():\n",
    "    network = torch.nn.Sequential(\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(784, 256),\n",
    "        torch.nn.Dropout(0.1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(256, 10),\n",
    "        torch.nn.Softmax(dim=1)\n",
    "    )\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActivationQuantizedLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, fnumber, bnumber, round_mode):\n",
    "        super(ActivationQuantizedLinear, self).__init__(in_features, out_features)\n",
    "        self.quant = Quantizer(forward_number=fnumber, forward_rounding=round_mode)\n",
    "        self.quant_b = Quantizer(backward_number=bnumber, backward_rounding=round_mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = super().forward(x)\n",
    "        x = self.quant_b(x)\n",
    "        return x\n",
    "\n",
    "def replace_linear_with_quantized(network, fnumber, bnumber, round_mode):\n",
    "    for i, m in enumerate(network.modules()):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            setattr(network, f\"{i}\", ActivationQuantizedLinear(m.in_features, m.out_features, fnumber, bnumber, round_mode))\n",
    "    return network\n",
    "\n",
    "network = get_network()\n",
    "network_q = replace_linear_with_quantized(deepcopy(network), fnumber, bnumber, args.round_mode)\n",
    "\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MasterWeightOptimizerWrapper():\n",
    "    def __init__(\n",
    "            self,\n",
    "            master_weight,\n",
    "            model_weight,\n",
    "            optimizer,\n",
    "            weight_quant=None,\n",
    "            grad_scaling=1.0,\n",
    "    ):\n",
    "        self.master_weight = master_weight\n",
    "        self.model_weight = model_weight\n",
    "        self.optimizer = optimizer\n",
    "        self.weight_quant = weight_quant\n",
    "        self.grad_scaling = grad_scaling\n",
    "        self.grad_clip = 100.0\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # --- for mix precision training ---\n",
    "    def model_grads_to_master_grads(self):\n",
    "        for model, master in zip(self.model_weight.parameters(), self.master_weight.parameters()):\n",
    "            if master.grad is None:\n",
    "                master.grad = master.data.new(*master.data.size())\n",
    "            master.grad.data.copy_(model.grad.data)\n",
    "\n",
    "    def master_grad_apply(self, fn):\n",
    "        for master in (self.master_weight.parameters()):\n",
    "            if master.grad is None:\n",
    "                master.grad = master.data.new(*master.data.size())\n",
    "            master.grad.data = fn(master.grad.data)\n",
    "\n",
    "    def master_params_to_model_params(self):\n",
    "        for model, master in zip(self.model_weight.parameters(), self.master_weight.parameters()):\n",
    "            model.data.copy_(self.weight_quant(master.data))\n",
    "\n",
    "    def train_on_batch(self, data, target):\n",
    "        self.model_weight.zero_grad()\n",
    "        output = self.model_weight(data)\n",
    "        loss = self.loss_fn(output, target)\n",
    "        loss = loss * self.grad_scaling\n",
    "        loss.backward()\n",
    "        self.model_grads_to_master_grads()\n",
    "        self.master_grad_apply(lambda x: x / self.grad_scaling)\n",
    "        nn.utils.clip_grad_norm_(self.master_weight.parameters(), self.grad_clip)\n",
    "        self.optimizer.step()\n",
    "        self.master_params_to_model_params()\n",
    "        return loss\n",
    "\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "device = \"cuda\"\n",
    "\n",
    "MnistDatasetTrain.data = MnistDataset.data.to(device)\n",
    "MnistDatasetTrain.targets = MnistDataset.targets.to(device)\n",
    "MnistDatasetTest.data = MnistDataset.data.to(device)\n",
    "MnistDatasetTest.targets = MnistDataset.targets.to(device)\n",
    "\n",
    "def grad_on_dataset(network, dataset):\n",
    "    network.train()\n",
    "    grad_norm = 0\n",
    "    network.zero_grad()\n",
    "    for data, target in dataset:\n",
    "        output = network(data)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        loss.backward()\n",
    "    for p in network.parameters():\n",
    "        grad_norm += p.grad.data.norm() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    network.eval()\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def test(network, dataset):\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataset:\n",
    "            i += 1\n",
    "            output = network(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    accuracy = correct / len(dataset.dataset) \n",
    "    avg_loss = total_loss / i\n",
    "    network.train()\n",
    "    return {\"acc\": accuracy, \"test_loss\": avg_loss}\n",
    "\n",
    "class MovingAvg():\n",
    "    def __init__(self, beta=0.9):\n",
    "        self.beta = beta\n",
    "        self.average = None\n",
    "\n",
    "    def update(self, value):\n",
    "        if self.average is None:\n",
    "            self.average = value\n",
    "        else:\n",
    "            self.average = self.beta * self.average + (1 - self.beta) * value\n",
    "\n",
    "    def get(self):\n",
    "        return self.average\n",
    "\n",
    "class MovingAvgStat():\n",
    "    def __init__(self, beta):\n",
    "        self.beta = beta\n",
    "        self.stats = {}\n",
    "\n",
    "    def add_value(self, stats):\n",
    "        for key in stats:\n",
    "            if key not in self.stats:\n",
    "                self.stats[key] = MovingAvg(self.beta)\n",
    "            self.stats[key].update(stats[key])\n",
    "\n",
    "    def get(self):\n",
    "        return {f\"{key}_mov_avg\": self.stats[key].get() for key in self.stats}\n",
    "\n",
    "def report_stats(network):\n",
    "    stats = {}\n",
    "    i = 0\n",
    "    for _, m in enumerate(network.modules()):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            stats[f\"{i}_w_norm\"] = m.weight.data.norm().item()\n",
    "            stats[f\"{i}_w_mean\"] = m.weight.data.mean().item()\n",
    "            stats[f\"{i}_w_std\"] = m.weight.data.std().item()\n",
    "            stats[f\"{i}_w_max\"] = m.weight.data.max().item()\n",
    "            stats[f\"{i}_w_min\"] = m.weight.data.min().item()\n",
    "            stats[f\"{i}_g_norm\"] = torch.sqrt((m.weight.grad.data ** 2).sum()).item() if m.weight.grad is not None else 0\n",
    "            stats[f\"{i}_g_mean\"] = m.weight.grad.data.mean().item() if m.weight.grad is not None else 0\n",
    "            stats[f\"{i}_g_std\"] = m.weight.grad.data.std().item() if m.weight.grad is not None else 0\n",
    "            i += 1\n",
    "    return stats\n",
    "\n",
    "def train(network, dataset, test_dataset, steps, lr=args.lr):\n",
    "    wandb.init(project=\"convergence_srsgd\", group=args.group, id=args.id)\n",
    "    wandb.watch(network, log='all', log_freq=10)\n",
    "\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=lr)\n",
    "    iteration = 0\n",
    "    loss_sum = 0\n",
    "    running_weight = \n",
    "    wrapper = MasterWeightOptimizerWrapper(network, model_weight=running_weight, optimizer=optimizer, weight_quant=qtorch.quant.quantizer(fnumber, forward_rounding=args.round_mode), grad_scaling=1.0)\n",
    "\n",
    "    bar = tqdm.tqdm(total=steps)\n",
    "    while True:\n",
    "        for data, target in (dataset):\n",
    "            if iteration >= steps:\n",
    "                return network\n",
    "            bar.update(1)\n",
    "            iteration += 1\n",
    "            loss = wrapper.train_on_batch(data, target)\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            if iteration % 10 == 0:\n",
    "                avg_loss = loss_sum / 10\n",
    "                wandb.log({\"iteration_loss\": avg_loss})\n",
    "                loss_sum = 0\n",
    "                stats = report_stats(network)\n",
    "                # stats_moving_average.add_value(stats)\n",
    "                wandb.log({\"0_g_norm\": stats[\"0_g_norm\"]})\n",
    "                wandb.log({\"1_g_norm\": stats[\"1_g_norm\"]})\n",
    "                # wandb.log({\"0_g_norm_mov_avg\": stats_moving_average.get()[\"0_g_norm_mov_avg\"]})\n",
    "                # wandb.log({\"1_g_norm_mov_avg\": stats_moving_average.get()[\"1_g_norm_mov_avg\"]})\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                wandb.log({\"grad_norm\": grad_on_dataset(network, dataset)})\n",
    "                test_metrics = test(network, test_dataset)\n",
    "                wandb.log(test_metrics)\n",
    "                bar.set_postfix(test_metrics)\n",
    "\n",
    "\n",
    "# %%\n",
    "import random\n",
    "class MyDataLoader():\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.idx = list(range(len(dataset)))\n",
    "        random.shuffle(self.idx)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        data = self.dataset.data\n",
    "        targets = self.dataset.targets\n",
    "        for i in range(0, len(data), self.batch_size):\n",
    "            idx = self.idx[i:i + self.batch_size]\n",
    "            if len(idx) == 0:\n",
    "                continue\n",
    "            yield data[idx].float(), targets[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "\n",
    "network = get_low_precision_network()\n",
    "print(network)\n",
    "\n",
    "train_loader = MyDataLoader(MnistDatasetTrain, args.batch_size)\n",
    "test_loader = MyDataLoader(MnistDatasetTest, 2500)\n",
    "network = network.to(device)\n",
    "train(network,train_loader, test_loader, steps=100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
