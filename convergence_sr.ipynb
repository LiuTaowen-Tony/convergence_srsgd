{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qtorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "MnistDataset = datasets.MNIST(\"./data\", download=True, train=False, transform=transform)\n",
    "\n",
    "MnistDatasetTrain =torch.utils.data.Subset(MnistDataset, range(7500)) \n",
    "MnistDatasetTest =torch.utils.data.Subset(MnistDataset, range(7500, 10000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "import qtorch\n",
    "from qtorch.quant import Quantizer\n",
    "\n",
    "fnumber = qtorch.FloatingPoint(5, 10)\n",
    "bnumber = qtorch.FloatingPoint(5, 10)\n",
    "\n",
    "def get_network():\n",
    "    network = torch.nn.Sequential(\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(784, 256),\n",
    "        torch.nn.Dropout(0.1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(256, 10),\n",
    "        torch.nn.Softmax(dim=1)\n",
    "    )\n",
    "    return network\n",
    "\n",
    "low_precision_network = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    Quantizer(forward_number=fnumber),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    Quantizer(backward_number=bnumber),\n",
    "    torch.nn.ReLU(),\n",
    "    Quantizer(forward_number=fnumber),\n",
    "    torch.nn.Linear(256, 10),\n",
    "    Quantizer(backward_number=bnumber),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "master_weight = deepcopy(low_precision_network)\n",
    "\n",
    "\n",
    "class MasterWeightOptimizerWrapper():\n",
    "    def __init__(\n",
    "            self,\n",
    "            master_weight,\n",
    "            model_weight,\n",
    "            optimizer,\n",
    "            weight_quant=None,\n",
    "            grad_scaling=1.0,\n",
    "    ):\n",
    "        self.master_weight = master_weight\n",
    "        self.model_weight = model_weight\n",
    "        self.optimizer = optimizer\n",
    "        self.weight_quant = weight_quant\n",
    "        self.grad_scaling = grad_scaling\n",
    "\n",
    "    # --- for mix precision training ---\n",
    "    def model_grads_to_master_grads(self):\n",
    "        for model, master in zip(self.model_weight.parameters(), self.master_weight.parameters()):\n",
    "            if master.grad is None:\n",
    "                master.grad = master.data.new(*master.data.size())\n",
    "            master.grad.data.copy_(self.grad_quant(model.grad.data))\n",
    "\n",
    "    def master_grad_apply(self, fn):\n",
    "        for master in (self.master_weight.parameters()):\n",
    "            if master.grad is None:\n",
    "                master.grad = master.data.new(*master.data.size())\n",
    "            master.grad.data = fn(master.grad.data)\n",
    "\n",
    "    def master_params_to_model_params(self):\n",
    "        for model, master in zip(self.model_weight.parameters(), self.master_weight.parameters()):\n",
    "            model.data.copy_(self.weight_quant(master.data))\n",
    "\n",
    "    def _apply_model_weights(self, model, quant_func):\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                m.weight.data = quant_func(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = quant_func(m.bias.data)\n",
    "\n",
    "    def train_on_loss(self, loss):\n",
    "        loss = loss * self.loss_scale\n",
    "        opt = self.optimizer\n",
    "        self.model_weight.zero_grad()\n",
    "        self.model_weight.backward(loss)\n",
    "        self.model_grads_to_master_grads()\n",
    "        self.master_grad_apply(lambda x: x / self.loss_scale)\n",
    "        nn.utils.clip_grad_norm_(self.master_weight.parameters(), self.grad_clip)\n",
    "        opt.step()\n",
    "        self.master_params_to_model_params()\n",
    "        self._apply_model_weights(self.model_weight, self.weight_quant)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "def test(network, dataset):\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataset:\n",
    "            output = network(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    accuracy = correct / len(dataset.dataset)  # ensure dataset.dataset is the correct reference\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "\n",
    "    return {\"acc\": accuracy, \"test_loss\": avg_loss}\n",
    "\n",
    "class MovingAvg():\n",
    "    def __init__(self, beta=0.9):\n",
    "        self.beta = beta\n",
    "        self.average = None\n",
    "\n",
    "    def update(self, value):\n",
    "        if self.average is None:\n",
    "            self.average = value\n",
    "        else:\n",
    "            self.average = self.beta * self.average + (1 - self.beta) * value\n",
    "\n",
    "    def get(self):\n",
    "        return self.average\n",
    "\n",
    "class MovingAvgStat():\n",
    "    def __init__(self, beta):\n",
    "        self.beta = beta\n",
    "        self.stats = {}\n",
    "\n",
    "    def add_value(self, stats):\n",
    "        for key in stats:\n",
    "            if key not in self.stats:\n",
    "                self.stats[key] = MovingAvg(self.beta)\n",
    "            self.stats[key].update(stats[key])\n",
    "\n",
    "    def get(self):\n",
    "        return {f\"{key}_mov_avg\": self.stats[key].get() for key in self.stats}\n",
    "\n",
    "def report_stats(network):\n",
    "    stats = {}\n",
    "    i = 0\n",
    "    for _, m in enumerate(network.modules()):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            stats[f\"{i}_w_norm\"] = m.weight.data.norm().item()\n",
    "            stats[f\"{i}_w_mean\"] = m.weight.data.mean().item()\n",
    "            stats[f\"{i}_w_std\"] = m.weight.data.std().item()\n",
    "            stats[f\"{i}_w_max\"] = m.weight.data.max().item()\n",
    "            stats[f\"{i}_w_min\"] = m.weight.data.min().item()\n",
    "            stats[f\"{i}_g_norm\"] = torch.sqrt((m.weight.grad.data ** 2).sum()).item() if m.weight.grad is not None else 0\n",
    "            stats[f\"{i}_g_mean\"] = m.weight.grad.data.mean().item() if m.weight.grad is not None else 0\n",
    "            stats[f\"{i}_g_std\"] = m.weight.grad.data.std().item() if m.weight.grad is not None else 0\n",
    "            i += 1\n",
    "    return stats\n",
    "\n",
    "def train(network, dataset, test_dataset, steps, lr=0.01):\n",
    "    wandb.init(project=\"convergence_srsgd\")\n",
    "    wandb.watch(network, log='all', log_freq=10)\n",
    "\n",
    "    network.train()\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    iteration = 0\n",
    "    loss_sum = 0\n",
    "    stats_moving_average = MovingAvgStat(0.9)\n",
    "\n",
    "    while True:\n",
    "        for data, target in dataset:\n",
    "            if iteration >= steps:\n",
    "                return network\n",
    "            iteration += 1\n",
    "            optimizer.zero_grad()\n",
    "            output = network(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            if iteration % 10 == 0:\n",
    "                avg_loss = loss_sum / 10\n",
    "                wandb.log({\"iteration_loss\": avg_loss})\n",
    "                loss_sum = 0\n",
    "                stats = report_stats(network)\n",
    "                # stats_moving_average.add_value(stats)\n",
    "                wandb.log({\"0_g_norm\": stats[\"0_g_norm\"]})\n",
    "                wandb.log({\"1_g_norm\": stats[\"1_g_norm\"]})\n",
    "                # wandb.log({\"0_g_norm_mov_avg\": stats_moving_average.get()[\"0_g_norm_mov_avg\"]})\n",
    "                # wandb.log({\"1_g_norm_mov_avg\": stats_moving_average.get()[\"1_g_norm_mov_avg\"]})\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                test_metrics = test(network, test_dataset)\n",
    "                wandb.log(test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:h7s57jq7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>0_g_norm</td><td>▇█▄▄▃▂▂▂▂▃▂▃▄▂▃▃▃▂▂▂▂▄▂▁▁▁▁▁▁▃▂▁▁▁▂▂▃▂▂▂</td></tr><tr><td>0_g_norm_mov_avg</td><td>▅█▄▃▂▂▂▁▂▃▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁</td></tr><tr><td>1_g_norm</td><td>▆█▆▄▅▃▃▃▃▅▃▄▆▄▃▃▅▃▃▂▂▅▃▁▂▂▁▂▁▃▃▂▁▁▂▂▃▂▂▂</td></tr><tr><td>1_g_norm_mov_avg</td><td>▄█▅▄▃▃▃▃▄▄▃▂▃▂▂▂▃▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▂▂▁▁▁</td></tr><tr><td>acc</td><td>▁▄▅▇▇▇▇▇▇███████████████████████████████</td></tr><tr><td>iteration_loss</td><td>█▅▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss</td><td>█▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>0_g_norm</td><td>0.13932</td></tr><tr><td>0_g_norm_mov_avg</td><td>0.11522</td></tr><tr><td>1_g_norm</td><td>0.10002</td></tr><tr><td>1_g_norm_mov_avg</td><td>0.07079</td></tr><tr><td>acc</td><td>0.9382</td></tr><tr><td>iteration_loss</td><td>1.53133</td></tr><tr><td>test_loss</td><td>1.53129</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-night-21</strong> at: <a href='https://wandb.ai/tony_t_liu/convergence_srsgd/runs/h7s57jq7' target=\"_blank\">https://wandb.ai/tony_t_liu/convergence_srsgd/runs/h7s57jq7</a><br/> View project at: <a href='https://wandb.ai/tony_t_liu/convergence_srsgd' target=\"_blank\">https://wandb.ai/tony_t_liu/convergence_srsgd</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240421_002354-h7s57jq7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:h7s57jq7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tony/convergence_sr/wandb/run-20240421_003904-adl6qjoc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tony_t_liu/convergence_srsgd/runs/adl6qjoc' target=\"_blank\">warm-shape-22</a></strong> to <a href='https://wandb.ai/tony_t_liu/convergence_srsgd' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tony_t_liu/convergence_srsgd' target=\"_blank\">https://wandb.ai/tony_t_liu/convergence_srsgd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tony_t_liu/convergence_srsgd/runs/adl6qjoc' target=\"_blank\">https://wandb.ai/tony_t_liu/convergence_srsgd/runs/adl6qjoc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network = get_network()\n",
    "train_loader = torch.utils.data.DataLoader(MnistDatasetTrain, batch_size=512, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(MnistDatasetTest, batch_size=512, shuffle=True)\n",
    "train(network,train_loader, test_loader, steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
