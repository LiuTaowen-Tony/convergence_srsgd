{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "\n",
    "from low_precision_utils import utils\n",
    "from low_precision_utils import metrics\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch.optim\n",
    "import qtorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIRED_STEPS = 1000\n",
    "DATASET_SIZE = 700\n",
    "BATCH_SIZE = 8\n",
    "STEPS_PER_EPOCH = DATASET_SIZE // BATCH_SIZE\n",
    "EPOCHS = DESIRED_STEPS // STEPS_PER_EPOCH + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(model):\n",
    "    return model.linear.network.weight.grad\n",
    "\n",
    "\n",
    "def diff_of_grad(wrapper, model_weight, master_weight, data, target):\n",
    "    # at the same traning step\n",
    "    # that is have the same training data and target\n",
    "    # we compute the gradient on full precison model\n",
    "    # then compute the same thing on low precision model (with different seed)\n",
    "    # we check if for each parameter the estimation is biased\n",
    "\n",
    "    # this time, we check the difference between activation quantise only\n",
    "    # and full precision model\n",
    "\n",
    "    master_weight.zero_grad()\n",
    "    reference_loss = master_weight.loss_acc(data, target)[\"loss\"]\n",
    "    reference_loss.backward()\n",
    "    reference_grad = get_grad(master_weight)\n",
    "    master_weight.zero_grad()\n",
    "\n",
    "    grad_estimation_samples = []\n",
    "    for i in range(100):\n",
    "        model_weight.zero_grad()\n",
    "        sample_loss = model_weight.loss_acc(data, target)[\"loss\"]\n",
    "        sample_loss.backward()\n",
    "        sample_grad = get_grad(model_weight)\n",
    "        grad_estimation_samples.append(np.array(sample_grad.detach().cpu()))\n",
    "        model_weight.zero_grad()\n",
    "\n",
    "    return np.array(reference_grad.detach().cpu()), grad_estimation_samples\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(5, 1)\n",
    "        self.input_size = (5,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "    def loss_acc(self, x, y):\n",
    "        output = self(x)\n",
    "        loss = nn.BCELoss()(output, y)\n",
    "        pred = output.round()\n",
    "        acc = pred.eq(y.view_as(pred)).sum().item() / len(y)\n",
    "        return {\"loss\": loss, \"acc\": acc}\n",
    "\n",
    "def loadSmallTableData(device):\n",
    "    train_data = pd.read_csv('../train_data.csv')\n",
    "    X_train = train_data.drop('purchased', axis=1).values\n",
    "    y_train = train_data['purchased'].values\n",
    "\n",
    "    # Load the test data\n",
    "    test_data = pd.read_csv('../test_data.csv')\n",
    "    X_test = test_data.drop('purchased', axis=1).values\n",
    "    y_test = test_data['purchased'].values\n",
    "\n",
    "    # Convert the data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    return X_train_tensor.to(device), y_train_tensor.to(device), X_test_tensor.to(device), y_test_tensor.to(device)\n",
    "\n",
    "def getBatches(X, y):\n",
    "    length = len(X)\n",
    "    idx = torch.randperm(length)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    for i in range(0, length, BATCH_SIZE):\n",
    "        yield X[i:i+BATCH_SIZE], y[i:i+BATCH_SIZE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "bit1 = qtorch.FloatingPoint(8, 1)\n",
    "X_train, y_train, X_test, y_test = loadSmallTableData(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.replace_linear_with_quantized(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_weight = model\n",
    "model_weight = copy.deepcopy(master_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight = utils.apply_number_format(model_weight, bit1, bit1, \"stochastic\", \"stochastic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(master_weight.parameters(), lr=0.03, momentum=0)\n",
    "scheduler = torch.optim.lr_scheduler.ConstantLR(opt)\n",
    "wrapper = utils.MasterWeightOptimizerWrapper(\n",
    "    master_weight,\n",
    "    model_weight,\n",
    "    opt,\n",
    "    scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [2:30:19,  9.02s/it, loss=0.35, acc=0.875, lr=0.03, grad_norm_entire=0.0334, zero_percentage=0]\n",
      "104it [00:01, 32.38it/s, loss=0.32, acc=1, lr=0.03, grad_norm_entire=0.256, zero_percentage=0]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1  test loss m 0.437   test acc m 0.923  training time 1.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "194it [00:03, 41.44it/s, loss=0.232, acc=1, lr=0.03, grad_norm_entire=0.152, zero_percentage=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  2  test loss m 0.332   test acc m 0.937  training time 3.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "281it [00:05, 36.92it/s, loss=0.237, acc=1, lr=0.03, grad_norm_entire=0.109, zero_percentage=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  3  test loss m 0.285   test acc m 0.940  training time 4.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "369it [00:06, 32.77it/s, loss=0.273, acc=0.875, lr=0.03, grad_norm_entire=0.085, zero_percentage=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  4  test loss m 0.259   test acc m 0.937  training time 6.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "458it [00:08, 40.08it/s, loss=0.134, acc=1, lr=0.03, grad_norm_entire=0.07, zero_percentage=0]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  5  test loss m 0.242   test acc m 0.937  training time 8.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "545it [00:09, 33.31it/s, loss=0.572, acc=0.625, lr=0.03, grad_norm_entire=0.0591, zero_percentage=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  6  test loss m 0.230   test acc m 0.937  training time 9.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "634it [00:11, 44.25it/s, loss=0.122, acc=1, lr=0.03, grad_norm_entire=0.0511, zero_percentage=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  7  test loss m 0.221   test acc m 0.937  training time 11.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "721it [00:12, 42.33it/s, loss=0.416, acc=0.875, lr=0.03, grad_norm_entire=0.0452, zero_percentage=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  8  test loss m 0.214   test acc m 0.940  training time 12.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "809it [00:14, 34.37it/s, loss=0.172, acc=1, lr=0.03, grad_norm_entire=0.0405, zero_percentage=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  9  test loss m 0.209   test acc m 0.940  training time 14.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "896it [00:16, 41.24it/s, loss=0.739, acc=0.625, lr=0.03, grad_norm_entire=0.0363, zero_percentage=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  10  test loss m 0.204   test acc m 0.940  training time 16.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "985it [00:17, 41.70it/s, loss=0.232, acc=0.875, lr=0.03, grad_norm_entire=0.0331, zero_percentage=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  11  test loss m 0.201   test acc m 0.937  training time 17.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:18, 56.43it/s, loss=0.235, acc=0.875, lr=0.03, grad_norm_entire=0.0331, zero_percentage=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  12  test loss m 0.200   test acc m 0.933  training time 18.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:28, 56.43it/s, loss=0.235, acc=0.875, lr=0.03, grad_norm_entire=0.0331, zero_percentage=0]"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def test(network, dataset):\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    i = 0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataset:\n",
    "            i += 1\n",
    "            n += len(data)\n",
    "            loss_acc = network.loss_acc(data, target)\n",
    "            total_loss += loss_acc[\"loss\"].item()\n",
    "            correct += loss_acc[\"acc\"] * len(data)\n",
    "    accuracy = correct / n\n",
    "    avg_loss = total_loss / i\n",
    "    network.train()\n",
    "    return {\"test_acc\": accuracy, \"test_loss\": avg_loss}\n",
    "\n",
    "training_time = 0\n",
    "import tqdm\n",
    "import time\n",
    "test_result_m = {}\n",
    "result_log = {}\n",
    "stepi=0\n",
    "bar = tqdm.tqdm()\n",
    "grad_real_estimations = []\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.perf_counter()\n",
    "    train_losses, train_accs = [], []\n",
    "    master_weight.train()\n",
    "    model_weight.train()\n",
    "\n",
    "    for X,y in getBatches(X_train,y_train):\n",
    "        stepi += 1\n",
    "        result_log.update(wrapper.train_on_batch(X, y))\n",
    "        bar.update(1)\n",
    "        bar.set_postfix(result_log)\n",
    "        result_log[\"lr\"] = opt.param_groups[0][\"lr\"]\n",
    "        if stepi >= DESIRED_STEPS:\n",
    "            test_result_m = test(master_weight, ((X_test, y_test),))\n",
    "            grad_entire = metrics.grad_on_dataset(master_weight, X_train, y_train)\n",
    "            result_log.update(grad_entire)\n",
    "            break\n",
    "    grad_real_estimations.append(diff_of_grad(wrapper, model_weight, master_weight, X, y))\n",
    "\n",
    "    # wrapper.master_params_to_model_params(quantize=False)\n",
    "    result_log.update(metrics.grad_on_dataset(master_weight, X_train, y_train))\n",
    "    training_time += time.perf_counter()-start_time\n",
    "    test_result_m = test(master_weight, ((X_test, y_test),))\n",
    "    print(f'epoch % 2d  test loss m %.3f   test acc m %.3f  training time %.2f'%(epoch+1, test_result_m[\"test_loss\"], test_result_m[\"test_acc\"], training_time))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_vectors(vs):\n",
    "    sum = np.zeros_like(vs[0])\n",
    "    for v in vs:\n",
    "        sum += v\n",
    "    return sum / len(vs)\n",
    "\n",
    "\n",
    "def var_of_vectors(vs):\n",
    "    mean = mean_of_vectors(vs)\n",
    "    sum = np.zeros_like(vs[0])\n",
    "    for v in vs:\n",
    "        sum += (v - mean) ** 2\n",
    "    return sum / len(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_stats(stats, i):\n",
    "    print(mean_of_vectors(stats[i][1]))\n",
    "    print(var_of_vectors(stats[i][1]))\n",
    "    print(stats[i][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06915283  0.02345215 -0.16585937  0.09882812 -0.17070313]]\n",
      "[[0.00011669 0.00095896 0.00019269 0.00174728 0.00058624]]\n",
      "[[-0.07003069  0.01795811 -0.16687551  0.08843183 -0.1695634 ]]\n"
     ]
    }
   ],
   "source": [
    "show_stats(grad_real_estimations, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# True vector\n",
    "def t_test(true_vector, estimated_vectors):\n",
    "    estimated_vectors = np.array(estimated_vectors)\n",
    "    print(estimated_vectors.shape)\n",
    "    # Calculate the average estimated vector\n",
    "    average_estimated_vector = np.mean(estimated_vectors, axis=0)\n",
    "\n",
    "    # Calculate the differences\n",
    "    differences = estimated_vectors - true_vector\n",
    "\n",
    "    # Perform Paired t-test\n",
    "    t_statistics, p_values = [], []\n",
    "    for i in range(true_vector.shape[0]):\n",
    "        t_statistic, p_value = stats.ttest_1samp(differences[:, i], 0)\n",
    "        t_statistics.append(t_statistic)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"True Vector: {true_vector}\")\n",
    "    print(f\"Average Estimated Vector: {average_estimated_vector}\")\n",
    "    print(f\"Paired t-test p-values: {p_values}\")\n",
    "    significant_differences = np.sum(np.array(p_values) < 0.05)\n",
    "    print(f\"Number of components with significant differences: {significant_differences} out of {true_vector.shape[0]}\")\n",
    "\n",
    "    # Additional Metrics\n",
    "    # Calculate MSE, RMSE, and Correlation Coefficient for each estimation\n",
    "    mses = np.mean(differences ** 2, axis=1)\n",
    "    rmses = np.sqrt(mses)\n",
    "    correlation_coefficients = [np.corrcoef(true_vector, estimated_vector)[0, 1] for estimated_vector in estimated_vectors]\n",
    "\n",
    "    # Calculate average MSE, RMSE, and Correlation Coefficient\n",
    "    average_mse = np.mean(mses)\n",
    "    average_rmse = np.mean(rmses)\n",
    "    average_correlation_coefficient = np.mean(correlation_coefficients)\n",
    "\n",
    "    print(f\"Average MSE: {average_mse}\")\n",
    "    print(f\"Average RMSE: {average_rmse}\")\n",
    "    print(f\"Average Correlation Coefficient: {average_correlation_coefficient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Vector: [[ 0.2585148   0.22784749 -0.203497   -0.1550882  -0.08595779]]\n",
      "Average Estimated Vector: [[ 0.26472655  0.23930664 -0.20344727 -0.158125   -0.08921875]]\n",
      "Paired t-test p-values: [array([0.00852391, 0.01760394, 0.98086672, 0.45991027, 0.08965652])]\n",
      "Number of components with significant differences: 2 out of 1\n",
      "Average MSE: 0.0010781824821606278\n",
      "Average RMSE: 0.024582665413618088\n",
      "Average Correlation Coefficient: 0.9936792916620897\n",
      "Mean Difference: 0.0022845782805234194\n",
      "Overall Bias Direction: overestimate\n",
      "********************\n",
      "True Vector: [[ 0.19589204  0.08953348 -0.14765157 -0.06848784  0.01696922]]\n",
      "Average Estimated Vector: [[ 0.19839722  0.09141602 -0.14982422 -0.07458984  0.0171875 ]]\n",
      "Paired t-test p-values: [array([0.18931731, 0.47785141, 0.09905809, 0.34159537, 0.9431865 ])]\n",
      "Number of components with significant differences: 0 out of 1\n",
      "Average MSE: 0.0012455843389034271\n",
      "Average RMSE: 0.024625740945339203\n",
      "Average Correlation Coefficient: 0.9754962307333724\n",
      "Mean Difference: -0.0007337314891628921\n",
      "Overall Bias Direction: underestimate\n",
      "********************\n",
      "True Vector: [[ 0.21326622 -0.04801209 -0.11515592  0.00150231  0.18023986]]\n",
      "Average Estimated Vector: [[ 0.21886231 -0.04658203 -0.11617187  0.00400635  0.18604492]]\n",
      "Paired t-test p-values: [array([0.00739315, 0.63095408, 0.24933996, 0.09620871, 0.06044315])]\n",
      "Number of components with significant differences: 1 out of 1\n",
      "Average MSE: 0.0005163569003343582\n",
      "Average RMSE: 0.016660518944263458\n",
      "Average Correlation Coefficient: 0.9932289325956006\n",
      "Mean Difference: 0.0028638578951358795\n",
      "Overall Bias Direction: overestimate\n",
      "********************\n",
      "True Vector: [[ 0.03474198  0.04346598 -0.13293898 -0.01514906 -0.07957412]]\n",
      "Average Estimated Vector: [[ 0.02920898  0.04639648 -0.13330078 -0.00939453 -0.08716308]]\n",
      "Paired t-test p-values: [array([0.00261985, 0.07950304, 0.76933119, 0.04640849, 0.03751435])]\n",
      "Number of components with significant differences: 3 out of 1\n",
      "Average MSE: 0.0005914185312576592\n",
      "Average RMSE: 0.01899373158812523\n",
      "Average Correlation Coefficient: 0.9578274432877673\n",
      "Mean Difference: -0.0009597446769475937\n",
      "Overall Bias Direction: underestimate\n",
      "********************\n",
      "True Vector: [[ 0.08428135  0.21819009 -0.05371544  0.05890493 -0.15749025]]\n",
      "Average Estimated Vector: [[ 0.08559936  0.21896698 -0.05334717  0.05895813 -0.15214477]]\n",
      "Paired t-test p-values: [array([0.37669045, 0.85021062, 0.6319575 , 0.96986588, 0.03064286])]\n",
      "Number of components with significant differences: 1 out of 1\n",
      "Average MSE: 0.0005515008815564215\n",
      "Average RMSE: 0.016649192199110985\n",
      "Average Correlation Coefficient: 0.9953640194131161\n",
      "Mean Difference: 0.001572370994836092\n",
      "Overall Bias Direction: overestimate\n",
      "********************\n",
      "True Vector: [[ 0.03065688 -0.04343018 -0.09935811 -0.15190412  0.01665559]]\n",
      "Average Estimated Vector: [[ 0.02986328 -0.04642578 -0.1        -0.15421875  0.01744141]]\n",
      "Paired t-test p-values: [array([0.20431845, 0.07421934, 0.37028498, 0.22541989, 0.13015084])]\n",
      "Number of components with significant differences: 0 out of 1\n",
      "Average MSE: 0.00015201917267404497\n",
      "Average RMSE: 0.008460231125354767\n",
      "Average Correlation Coefficient: 0.9939505415933341\n",
      "Mean Difference: -0.001191981602460146\n",
      "Overall Bias Direction: underestimate\n",
      "********************\n",
      "True Vector: [[-0.0335769   0.01163445 -0.0051056   0.18941317 -0.0445272 ]]\n",
      "Average Estimated Vector: [[-0.03428223  0.01309875 -0.00442383  0.19285461 -0.04560974]]\n",
      "Paired t-test p-values: [array([0.72201223, 0.2422246 , 0.3760248 , 0.49379953, 0.30319056])]\n",
      "Number of components with significant differences: 0 out of 1\n",
      "Average MSE: 0.0006416859105229378\n",
      "Average RMSE: 0.017808737233281136\n",
      "Average Correlation Coefficient: 0.9928005278570424\n",
      "Mean Difference: 0.0007599282544106245\n",
      "Overall Bias Direction: overestimate\n",
      "********************\n",
      "True Vector: [[ 0.03392702  0.00902268 -0.07248849  0.01422911 -0.01575077]]\n",
      "Average Estimated Vector: [[ 0.0355664   0.00263672 -0.07519531  0.01536133 -0.01263916]]\n",
      "Paired t-test p-values: [array([0.01745315, 0.00094361, 0.03710021, 0.1052996 , 0.0174141 ])]\n",
      "Number of components with significant differences: 4 out of 1\n",
      "Average MSE: 0.00016568871797062457\n",
      "Average RMSE: 0.009560258127748966\n",
      "Average Correlation Coefficient: 0.9630555274595416\n",
      "Mean Difference: -0.0006419130368158221\n",
      "Overall Bias Direction: underestimate\n",
      "********************\n",
      "True Vector: [[ 0.08083577  0.12886673 -0.08185227 -0.17170583 -0.09103275]]\n",
      "Average Estimated Vector: [[ 0.07853515  0.12796874 -0.081875   -0.170625   -0.09118652]]\n",
      "Paired t-test p-values: [array([0.0303116 , 0.63392252, 0.97576553, 0.72569569, 0.92050925])]\n",
      "Number of components with significant differences: 1 out of 1\n",
      "Average MSE: 0.0003377629618626088\n",
      "Average RMSE: 0.013660234399139881\n",
      "Average Correlation Coefficient: 0.9949064341490528\n",
      "Mean Difference: -0.0004588548617903143\n",
      "Overall Bias Direction: underestimate\n",
      "********************\n",
      "True Vector: [[-0.07564196 -0.04614413 -0.01900901  0.15187706 -0.03582963]]\n",
      "Average Estimated Vector: [[-0.07529236 -0.04567017 -0.01816132  0.15308107 -0.03574509]]\n",
      "Paired t-test p-values: [array([0.82613384, 0.52519839, 0.11905852, 0.61233442, 0.90996587])]\n",
      "Number of components with significant differences: 0 out of 1\n",
      "Average MSE: 0.0001891677820822224\n",
      "Average RMSE: 0.010060452856123447\n",
      "Average Correlation Coefficient: 0.9980379269192176\n",
      "Mean Difference: 0.0005919632967561483\n",
      "Overall Bias Direction: overestimate\n",
      "********************\n",
      "True Vector: [[-0.06480105 -0.18877898  0.05987024 -0.17191914  0.1508014 ]]\n",
      "Average Estimated Vector: [[-0.06401855 -0.19093506  0.06078858 -0.1724878   0.15255065]]\n",
      "Paired t-test p-values: [array([0.60385122, 0.48146535, 0.62622581, 0.82663801, 0.4628499 ])]\n",
      "Number of components with significant differences: 0 out of 1\n",
      "Average MSE: 0.0005452467012219131\n",
      "Average RMSE: 0.017565306276082993\n",
      "Average Correlation Coefficient: 0.9946907073854311\n",
      "Mean Difference: 0.0001450714626116678\n",
      "Overall Bias Direction: overestimate\n",
      "********************\n",
      "True Vector: [[ 0.00784131  0.04368623 -0.03370348 -0.13495639 -0.05196667]]\n",
      "Average Estimated Vector: [[ 0.00425415  0.0433049  -0.03343017 -0.1344529  -0.05206902]]\n",
      "Paired t-test p-values: [array([0.01608841, 0.77961267, 0.49432478, 0.77676035, 0.90987063])]\n",
      "Number of components with significant differences: 1 out of 1\n",
      "Average MSE: 0.00016306385805364698\n",
      "Average RMSE: 0.008992948569357395\n",
      "Average Correlation Coefficient: 0.9916555905903135\n",
      "Mean Difference: -0.0006588061805814505\n",
      "Overall Bias Direction: underestimate\n",
      "********************\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "(1, 5)\n",
      "(100, 1, 5)\n",
      "[ 0.0004569   0.00063338 -0.00031523  0.0003043   0.00040929]\n",
      "\n",
      "Combined t-test result: t-statistic = 1.8355142467655348, p-value = 0.14032671410845132\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def perform_big_t_test(true_vectors, estimated_vectors_list):\n",
    "    \"\"\"\n",
    "    Perform a combined t-test on multiple true vectors and their estimated vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    true_vectors (list of np.array): List of true vectors.\n",
    "    estimated_vectors_list (list of list of np.array): List of lists of estimated vectors.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing the t-statistic and p-value of the combined t-test.\n",
    "    \"\"\"\n",
    "    all_differences = []\n",
    "\n",
    "    for true_vector, estimated_vectors in zip(true_vectors, estimated_vectors_list):\n",
    "        estimated_vectors = np.array(estimated_vectors)\n",
    "        differences = estimated_vectors - true_vector\n",
    "        all_differences.extend(differences)\n",
    "\n",
    "    all_differences = np.array(all_differences)\n",
    "    combined_differences = np.mean(all_differences, axis=0)\n",
    "    \n",
    "    combined_differences = combined_differences[0,:]\n",
    "    print(combined_differences)\n",
    "    t_statistic, p_value = stats.ttest_1samp(combined_differences, 0)\n",
    "\n",
    "    return {\n",
    "        't_statistic': t_statistic,\n",
    "        'p_value': p_value\n",
    "    }\n",
    "\n",
    "def t_test(true_vector, estimated_vectors):\n",
    "    \"\"\"\n",
    "    Perform t-test and calculate metrics to evaluate the estimation quality.\n",
    "    \n",
    "    Parameters:\n",
    "    true_vector (np.array): The true values of the vector.\n",
    "    estimated_vectors (list): A list of estimated vectors.\n",
    "    \"\"\"\n",
    "    estimated_vectors = np.array(estimated_vectors)\n",
    "    \n",
    "    metrics = calculate_metrics(true_vector, estimated_vectors)\n",
    "    print_metrics(true_vector, metrics)\n",
    "\n",
    "\n",
    "def calculate_metrics(true_vector, estimated_vectors):\n",
    "    \"\"\"\n",
    "    Calculate various metrics to evaluate the estimation quality.\n",
    "    \n",
    "    Parameters:\n",
    "    true_vector (np.array): The true values of the vector.\n",
    "    estimated_vectors (np.array): The estimated vectors.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing average estimated vector, MSE, RMSE, and correlation coefficients.\n",
    "    \"\"\"\n",
    "    # Calculate the average estimated vector\n",
    "    average_estimated_vector = np.mean(estimated_vectors, axis=0)\n",
    "    \n",
    "    # Calculate the differences\n",
    "    differences = estimated_vectors - true_vector\n",
    "    \n",
    "    # Perform Paired t-test\n",
    "    t_statistics, p_values = [], []\n",
    "    for i in range(true_vector.shape[0]):\n",
    "        t_statistic, p_value = stats.ttest_1samp(differences[:, i], 0)\n",
    "        t_statistics.append(t_statistic)\n",
    "        p_values.append(p_value)\n",
    "    \n",
    "    # Calculate MSE, RMSE, and Correlation Coefficient for each estimation\n",
    "    mses = np.mean(differences ** 2, axis=1)\n",
    "    rmses = np.sqrt(mses)\n",
    "    correlation_coefficients = [np.corrcoef(true_vector, estimated_vector)[0, 1] for estimated_vector in estimated_vectors]\n",
    "    \n",
    "    # Calculate average MSE, RMSE, and Correlation Coefficient\n",
    "    average_mse = np.mean(mses)\n",
    "    average_rmse = np.mean(rmses)\n",
    "    average_correlation_coefficient = np.mean(correlation_coefficients)\n",
    "    \n",
    "    return {\n",
    "        'average_estimated_vector': average_estimated_vector,\n",
    "        'p_values': p_values,\n",
    "        'significant_differences': np.sum(np.array(p_values) < 0.05),\n",
    "        'average_mse': average_mse,\n",
    "        'average_rmse': average_rmse,\n",
    "        'average_correlation_coefficient': average_correlation_coefficient,\n",
    "        'mean_difference': np.mean(differences),\n",
    "        'direction': 'overestimate' if np.mean(differences) > 0 else 'underestimate' if np.mean(differences) < 0 else 'no bias'\n",
    "    }\n",
    "\n",
    "def print_metrics(true_vector, metrics):\n",
    "    \"\"\"\n",
    "    Print the calculated metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    true_vector (np.array): The true values of the vector.\n",
    "    metrics (dict): A dictionary containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    print(f\"True Vector: {true_vector}\")\n",
    "    print(f\"Average Estimated Vector: {metrics['average_estimated_vector']}\")\n",
    "    print(f\"Paired t-test p-values: {metrics['p_values']}\")\n",
    "    print(f\"Number of components with significant differences: {metrics['significant_differences']} out of {true_vector.shape[0]}\")\n",
    "    print(f\"Average MSE: {metrics['average_mse']}\")\n",
    "    print(f\"Average RMSE: {metrics['average_rmse']}\")\n",
    "    print(f\"Average Correlation Coefficient: {metrics['average_correlation_coefficient']}\")\n",
    "    print(f\"Mean Difference: {metrics['mean_difference']}\")\n",
    "    print(f\"Overall Bias Direction: {metrics['direction']}\")\n",
    "\n",
    "\n",
    "for true_vector, estimated_vectors in grad_real_estimations:\n",
    "    t_test(true_vector, estimated_vectors)\n",
    "    print(\"*\"*20)\n",
    "\n",
    "# Perform a combined t-test across all vectors\n",
    "true_vectors = [true_vector for true_vector, _ in grad_real_estimations]\n",
    "estimated_vectors_list = [estimated_vectors for _, estimated_vectors in grad_real_estimations]\n",
    "\n",
    "big_t_test_result = perform_big_t_test(true_vectors, estimated_vectors_list)\n",
    "print(f\"\\nCombined t-test result: t-statistic = {big_t_test_result['t_statistic']}, p-value = {big_t_test_result['p_value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mttest_1samp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpopmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnan_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'propagate'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0malternative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'two-sided'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Calculate the T-test for the mean of ONE group of scores.\n",
      "\n",
      "This is a test for the null hypothesis that the expected value\n",
      "(mean) of a sample of independent observations `a` is equal to the given\n",
      "population mean, `popmean`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "a : array_like\n",
      "    Sample observations.\n",
      "popmean : float or array_like\n",
      "    Expected value in null hypothesis. If array_like, then its length along\n",
      "    `axis` must equal 1, and it must otherwise be broadcastable with `a`.\n",
      "axis : int or None, default: 0\n",
      "    If an int, the axis of the input along which to compute the statistic.\n",
      "    The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
      "    corresponding element of the output.\n",
      "    If ``None``, the input will be raveled before computing the statistic.\n",
      "nan_policy : {'propagate', 'omit', 'raise'}\n",
      "    Defines how to handle input NaNs.\n",
      "    \n",
      "    - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
      "      which the  statistic is computed, the corresponding entry of the output\n",
      "      will be NaN.\n",
      "    - ``omit``: NaNs will be omitted when performing the calculation.\n",
      "      If insufficient data remains in the axis slice along which the\n",
      "      statistic is computed, the corresponding entry of the output will be\n",
      "      NaN.\n",
      "    - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
      "alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "    Defines the alternative hypothesis.\n",
      "    The following options are available (default is 'two-sided'):\n",
      "    \n",
      "    * 'two-sided': the mean of the underlying distribution of the sample\n",
      "      is different than the given population mean (`popmean`)\n",
      "    * 'less': the mean of the underlying distribution of the sample is\n",
      "      less than the given population mean (`popmean`)\n",
      "    * 'greater': the mean of the underlying distribution of the sample is\n",
      "      greater than the given population mean (`popmean`)\n",
      "keepdims : bool, default: False\n",
      "    If this is set to True, the axes which are reduced are left\n",
      "    in the result as dimensions with size one. With this option,\n",
      "    the result will broadcast correctly against the input array.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "result : `~scipy.stats._result_classes.TtestResult`\n",
      "    An object with the following attributes:\n",
      "    \n",
      "    statistic : float or array\n",
      "        The t-statistic.\n",
      "    pvalue : float or array\n",
      "        The p-value associated with the given alternative.\n",
      "    df : float or array\n",
      "        The number of degrees of freedom used in calculation of the\n",
      "        t-statistic; this is one less than the size of the sample\n",
      "        (``a.shape[axis]``).\n",
      "    \n",
      "        .. versionadded:: 1.10.0\n",
      "    \n",
      "    The object also has the following method:\n",
      "    \n",
      "    confidence_interval(confidence_level=0.95)\n",
      "        Computes a confidence interval around the population\n",
      "        mean for the given confidence level.\n",
      "        The confidence interval is returned in a ``namedtuple`` with\n",
      "        fields `low` and `high`.\n",
      "    \n",
      "        .. versionadded:: 1.10.0\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The statistic is calculated as ``(np.mean(a) - popmean)/se``, where\n",
      "``se`` is the standard error. Therefore, the statistic will be positive\n",
      "when the sample mean is greater than the population mean and negative when\n",
      "the sample mean is less than the population mean.\n",
      "\n",
      "Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
      "code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
      "this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
      "rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
      "arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
      "masked array with ``mask=False``.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Suppose we wish to test the null hypothesis that the mean of a population\n",
      "is equal to 0.5. We choose a confidence level of 99%; that is, we will\n",
      "reject the null hypothesis in favor of the alternative if the p-value is\n",
      "less than 0.01.\n",
      "\n",
      "When testing random variates from the standard uniform distribution, which\n",
      "has a mean of 0.5, we expect the data to be consistent with the null\n",
      "hypothesis most of the time.\n",
      "\n",
      ">>> import numpy as np\n",
      ">>> from scipy import stats\n",
      ">>> rng = np.random.default_rng()\n",
      ">>> rvs = stats.uniform.rvs(size=50, random_state=rng)\n",
      ">>> stats.ttest_1samp(rvs, popmean=0.5)\n",
      "TtestResult(statistic=2.456308468440, pvalue=0.017628209047638, df=49)\n",
      "\n",
      "As expected, the p-value of 0.017 is not below our threshold of 0.01, so\n",
      "we cannot reject the null hypothesis.\n",
      "\n",
      "When testing data from the standard *normal* distribution, which has a mean\n",
      "of 0, we would expect the null hypothesis to be rejected.\n",
      "\n",
      ">>> rvs = stats.norm.rvs(size=50, random_state=rng)\n",
      ">>> stats.ttest_1samp(rvs, popmean=0.5)\n",
      "TtestResult(statistic=-7.433605518875, pvalue=1.416760157221e-09, df=49)\n",
      "\n",
      "Indeed, the p-value is lower than our threshold of 0.01, so we reject the\n",
      "null hypothesis in favor of the default \"two-sided\" alternative: the mean\n",
      "of the population is *not* equal to 0.5.\n",
      "\n",
      "However, suppose we were to test the null hypothesis against the\n",
      "one-sided alternative that the mean of the population is *greater* than\n",
      "0.5. Since the mean of the standard normal is less than 0.5, we would not\n",
      "expect the null hypothesis to be rejected.\n",
      "\n",
      ">>> stats.ttest_1samp(rvs, popmean=0.5, alternative='greater')\n",
      "TtestResult(statistic=-7.433605518875, pvalue=0.99999999929, df=49)\n",
      "\n",
      "Unsurprisingly, with a p-value greater than our threshold, we would not\n",
      "reject the null hypothesis.\n",
      "\n",
      "Note that when working with a confidence level of 99%, a true null\n",
      "hypothesis will be rejected approximately 1% of the time.\n",
      "\n",
      ">>> rvs = stats.uniform.rvs(size=(100, 50), random_state=rng)\n",
      ">>> res = stats.ttest_1samp(rvs, popmean=0.5, axis=1)\n",
      ">>> np.sum(res.pvalue < 0.01)\n",
      "1\n",
      "\n",
      "Indeed, even though all 100 samples above were drawn from the standard\n",
      "uniform distribution, which *does* have a population mean of 0.5, we would\n",
      "mistakenly reject the null hypothesis for one of them.\n",
      "\n",
      "`ttest_1samp` can also compute a confidence interval around the population\n",
      "mean.\n",
      "\n",
      ">>> rvs = stats.norm.rvs(size=50, random_state=rng)\n",
      ">>> res = stats.ttest_1samp(rvs, popmean=0)\n",
      ">>> ci = res.confidence_interval(confidence_level=0.95)\n",
      ">>> ci\n",
      "ConfidenceInterval(low=-0.3193887540880017, high=0.2898583388980972)\n",
      "\n",
      "The bounds of the 95% confidence interval are the\n",
      "minimum and maximum values of the parameter `popmean` for which the\n",
      "p-value of the test would be 0.05.\n",
      "\n",
      ">>> res = stats.ttest_1samp(rvs, popmean=ci.low)\n",
      ">>> np.testing.assert_allclose(res.pvalue, 0.05)\n",
      ">>> res = stats.ttest_1samp(rvs, popmean=ci.high)\n",
      ">>> np.testing.assert_allclose(res.pvalue, 0.05)\n",
      "\n",
      "Under certain assumptions about the population from which a sample\n",
      "is drawn, the confidence interval with confidence level 95% is expected\n",
      "to contain the true population mean in 95% of sample replications.\n",
      "\n",
      ">>> rvs = stats.norm.rvs(size=(50, 1000), loc=1, random_state=rng)\n",
      ">>> res = stats.ttest_1samp(rvs, popmean=0)\n",
      ">>> ci = res.confidence_interval()\n",
      ">>> contains_pop_mean = (ci.low < 1) & (ci.high > 1)\n",
      ">>> contains_pop_mean.sum()\n",
      "953\n",
      "\u001b[0;31mFile:\u001b[0m      ~/convergence_srsgd/.venv/lib64/python3.9/site-packages/scipy/stats/_stats_py.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "stats.ttest_1samp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1, 5)\n",
      "True Vector: [[ 0.32465073  0.1262475  -0.2092338  -0.19066718  0.06783204]]\n",
      "Average Estimated Vector: [[ 0.32726562  0.12816407 -0.20972656 -0.1953125   0.07077149]]\n",
      "Paired t-test p-values: [array([0.33836574, 0.20539645, 0.77088319, 0.16036817, 0.35392142])]\n",
      "Number of components with significant differences: 0 out of 1\n",
      "Average MSE: 0.0006663842359557748\n",
      "Average RMSE: 0.019829940050840378\n",
      "Average Correlation Coefficient: 0.9961007561448865\n",
      "**********\n",
      "(100, 1, 5)\n",
      "True Vector: [[-0.00974826 -0.06938097 -0.11721987 -0.12058986 -0.00532948]]\n",
      "Average Estimated Vector: [[-0.00894531 -0.07382812 -0.11748535 -0.1209082  -0.00269531]]\n",
      "Paired t-test p-values: [array([0.76312473, 0.11252516, 0.85586587, 0.92282292, 0.07047263])]\n",
      "Number of components with significant differences: 0 out of 1\n",
      "Average MSE: 0.0005938939866609871\n",
      "Average RMSE: 0.01872951351106167\n",
      "Average Correlation Coefficient: 0.944159301396356\n",
      "**********\n",
      "(100, 1, 5)\n",
      "True Vector: [[-0.07003069  0.01795811 -0.16687551  0.08843183 -0.1695634 ]]\n",
      "Average Estimated Vector: [[-0.06915283  0.02345215 -0.16585937  0.09882812 -0.17070313]]\n",
      "Paired t-test p-values: [array([0.42070264, 0.08060312, 0.46812347, 0.01503653, 0.64055581])]\n",
      "Number of components with significant differences: 1 out of 1\n",
      "Average MSE: 0.0007486481918022037\n",
      "Average RMSE: 0.019687069579958916\n",
      "Average Correlation Coefficient: 0.9847924776534356\n",
      "**********\n",
      "(100, 1, 5)\n",
      "True Vector: [[ 0.02355878 -0.06636126 -0.1051937   0.03961908  0.0287477 ]]\n",
      "Average Estimated Vector: [[ 0.02597656 -0.07122803 -0.10733399  0.04030762  0.03065651]]\n",
      "Paired t-test p-values: [array([3.10119975e-04, 2.45964939e-03, 3.24296423e-02, 6.42208612e-01,\n",
      "       2.99790748e-03])]\n",
      "Number of components with significant differences: 4 out of 1\n",
      "Average MSE: 0.00013475683226715773\n",
      "Average RMSE: 0.00847788155078888\n",
      "Average Correlation Coefficient: 0.9924255127929793\n",
      "**********\n",
      "(100, 1, 5)\n",
      "True Vector: [[ 0.06668425  0.04303477 -0.09168632  0.01868218 -0.02802067]]\n",
      "Average Estimated Vector: [[ 0.06494141  0.04701172 -0.091875    0.01769531 -0.03302734]]\n",
      "Paired t-test p-values: [array([0.38785221, 0.00347076, 0.85974458, 0.5452918 , 0.07752822])]\n",
      "Number of components with significant differences: 1 out of 1\n",
      "Average MSE: 0.00035465139080770314\n",
      "Average RMSE: 0.014935504645109177\n",
      "Average Correlation Coefficient: 0.9721926001318789\n",
      "**********\n",
      "(100, 1, 5)\n",
      "True Vector: [[ 0.12605135  0.03676696 -0.09048673  0.2105931   0.03303409]]\n",
      "Average Estimated Vector: [[ 0.12734863  0.03816528 -0.09154297  0.21006347  0.03182617]]\n",
      "Paired t-test p-values: [array([0.40865103, 0.36912797, 0.26456345, 0.88588689, 0.46951313])]\n",
      "Number of components with significant differences: 0 out of 1\n",
      "Average MSE: 0.0004378880839794874\n",
      "Average RMSE: 0.01478311512619257\n",
      "Average Correlation Coefficient: 0.9921663171417476\n",
      "**********\n",
      "(100, 1, 5)\n",
      "True Vector: [[ 0.03938702  0.09848914 -0.06290369  0.00093452 -0.0903108 ]]\n",
      "Average Estimated Vector: [[ 0.03786621  0.10181641 -0.06274536  0.00418701 -0.09415772]]\n",
      "Paired t-test p-values: [array([0.46526289, 0.16549613, 0.89867873, 0.00975934, 0.25924982])]\n",
      "Number of components with significant differences: 1 out of 1\n",
      "Average MSE: 0.0004934778553433716\n",
      "Average RMSE: 0.017370540648698807\n",
      "Average Correlation Coefficient: 0.9780438205702875\n",
      "**********\n",
      "(100, 1, 5)\n",
      "True Vector: [[-0.03116461 -0.05496927 -0.06357914  0.07096206 -0.01023928]]\n",
      "Average Estimated Vector: [[-0.03187103 -0.05558106 -0.06283692  0.07156982 -0.0107605 ]]\n",
      "Paired t-test p-values: [array([0.44301355, 0.73016908, 0.32447899, 0.71868015, 0.369242  ])]\n",
      "Number of components with significant differences: 0 out of 1\n",
      "Average MSE: 0.00015279522631317377\n",
      "Average RMSE: 0.008770197629928589\n",
      "Average Correlation Coefficient: 0.9864542010937373\n",
      "**********\n",
      "(100, 1, 5)\n",
      "True Vector: [[-0.23732878 -0.26200026 -0.02956089  0.3514989   0.01440124]]\n",
      "Average Estimated Vector: [[-0.24326156 -0.26753753 -0.03082397  0.36086425  0.01676331]]\n",
      "Paired t-test p-values: [array([0.43142878, 0.26893264, 0.04434676, 0.18548472, 0.09978165])]\n",
      "Number of components with significant differences: 1 out of 1\n",
      "Average MSE: 0.0026645774487406015\n",
      "Average RMSE: 0.03135227411985397\n",
      "Average Correlation Coefficient: 0.9966833670280013\n",
      "**********\n",
      "(100, 1, 5)\n",
      "True Vector: [[ 0.0263384  -0.04527347  0.03862909  0.43702298  0.08744639]]\n",
      "Average Estimated Vector: [[ 0.0180542  -0.04649841  0.03758545  0.44743773  0.08043426]]\n",
      "Paired t-test p-values: [array([0.04987165, 0.69773139, 0.07548276, 0.33025351, 0.1682429 ])]\n",
      "Number of components with significant differences: 1 out of 1\n",
      "Average MSE: 0.0033417867962270975\n",
      "Average RMSE: 0.03808972239494324\n",
      "Average Correlation Coefficient: 0.9845744386163165\n",
      "**********\n",
      "(100, 1, 5)\n",
      "True Vector: [[-0.12523504 -0.115408    0.04840585  0.31054968  0.01985087]]\n",
      "Average Estimated Vector: [[-0.12727295 -0.12260498  0.05107422  0.31498075  0.0202536 ]]\n",
      "Paired t-test p-values: [array([0.78839567, 0.10297932, 0.12009894, 0.37331715, 0.61751065])]\n",
      "Number of components with significant differences: 0 out of 1\n",
      "Average MSE: 0.0020867877174168825\n",
      "Average RMSE: 0.03204435855150223\n",
      "Average Correlation Coefficient: 0.9866358877551417\n",
      "**********\n",
      "(100, 1, 5)\n",
      "True Vector: [[-0.08917266 -0.16574387 -0.05727343 -0.0270391   0.04465668]]\n",
      "Average Estimated Vector: [[-0.08905762 -0.16606873 -0.05745117 -0.03076671  0.04436127]]\n",
      "Paired t-test p-values: [array([0.96751908, 0.90530257, 0.77102917, 0.19807259, 0.83892395])]\n",
      "Number of components with significant differences: 0 out of 1\n",
      "Average MSE: 0.0005197588470764458\n",
      "Average RMSE: 0.016406698152422905\n",
      "Average Correlation Coefficient: 0.9717197135234038\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(grad_real_estimations)):\n",
    "    t_test(grad_real_estimations[i][0], grad_real_estimations[i][1])\n",
    "    print(\"*\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
